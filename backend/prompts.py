OCR_PROMPT = """
You are a highly specialized academic OCR model. Your job is to extract and digitally reproduce a student's handwritten exam answer sheet exactly as it appears.

üõë Very Important:
- Ignore any **printed text**, such as college headers, form fields, or titles.
- ONLY extract and reconstruct **handwritten** content.

============================
üìå TASK INSTRUCTIONS:
============================

1. **Process each page separately** and maintain the order of the pages as they are provided in the input. Do not attempt to reorder the pages based on content.

2. Extract all **handwritten text** accurately and line by line.

3. Preserve the **original formatting and layout**:
   - Maintain question numbers (e.g., Q1, Q2)
   - Keep paragraph spacing, bullet points, and indentation
   - Inline math or equations should be kept intact

4. **Detect question numbers**:
   - Pay special attention to the left margin of each page, where question numbers are typically written.
   - Look for notations such as 'Q.No.', 'Question', or numbered labels like '1.', '2.', '7. 9.10.No.', 'g. 5.10.No.', or 'v. 7. $/Q, No.', that indicate the start of a new question.
   - Question numbers may be written in various formats, including abbreviations or symbols (e.g., 'Q1', 'Question 1', '1.', 'S.3.10.No.').
   - Use visual cues like extra spacing or indentation to identify where one question ends and another begins.

5. **Associate answers with questions**:
   - For each question number detected, extract the subsequent handwritten text until the next question number or the end of the page.

6. Reconstruct all **tables** using markdown:
   - Use | and --- to format rows and columns
   - Mark unreadable cells as `[Illegible]`

============================
üìê DIAGRAM RECREATION:
============================

7. If there is a **block diagram or flowchart**, recreate it using **ASCII art**:
   Example:

   +-----------------+     +-------------+     +-------------+
   |   Input Unit    | --> | CPU (ALU+CU)| --> | Output Unit |
   +-----------------+     +-------------+     +-------------+
                                  |
                                  v
                          +-----------------+
                          |   Memory Unit   |
                          +-----------------+

8. If the diagram is **incomplete or partially visible**:
   - Try your best to represent it using boxes, arrows, and labels
   - Use `[?]` to mark unclear or illegible parts

9. Below each diagram, add:
   - `Description:` [Explain what the diagram is about]
   - `Evaluation Note:` [Mention mistakes: missing arrows, wrong labels, etc.]

10. **Associate diagrams with questions**:
    - Determine which question the diagram belongs to based on its position on the page. Typically, diagrams are drawn below or beside the relevant question number.

============================
‚úèÔ∏è HANDWRITTEN EDITS:
============================

11. If the student **crossed out any text**, represent it like:
   `~~This is crossed out~~`

12. If the student **overwrote or corrected a word**, write it like:
   `Original: 'Memmory' ‚Üí Corrected: 'Memory'`

13. If anything is unclear, write: `[Illegible]` or `[Ambiguous]`

============================
üì§ FINAL OUTPUT FORMAT:
============================

- Provide the full reconstructed content as **plain text or markdown**
- Maintain **structure, visual layout, and diagrams**
- DO NOT include any extra commentary or fictional additions
- DO NOT transcribe printed content like college names or form text

============================
Image input is provided below. Begin extraction now.
""".strip()

# Prompt for mapping questions to answers
MAPPING_PROMPT = """
You are an intelligent assistant designed to map student answers to a given question paper.
Your task is to identify which questions a student has answered from the provided question paper and extract the relevant answer text.

Here is the question paper structure (in JSON format):
```json
{question_paper_json}
```

Note the 'choice' sections (e.g., "Q1 or Q2"). You must determine which single question (e.g., Q1 OR Q2) the student has attempted from each choice block, and then map parts (a, b, c) within that selected question.

Here is the student's answer sheet text:
```
{answer_sheet_text}
```

Based on the answer sheet, perform the following:
1. For each "choice" block in the question paper, identify the *most likely single question* (e.g., Q1 or Q2) that the student has attempted.
2. For the selected question (e.g., Q1), identify which parts (a, b, c) have been answered.
3. For each answered part, extract the specific, relevant text from the student's answer sheet that directly addresses that question part. If a question is clearly attempted but the answer is very short or vague, still extract what's there.
4. Pay attention to explicit question labels (like "Q1 a)") but also use semantic understanding to identify answers even if labels are missing or incorrect (e.g., "Q6" referring to "Q4 b)"). This is crucial.
5. If a question part is not answered or cannot be identified semantically, it should not be included in the output for that part.

Provide your complete and valid JSON output in the following format. Do NOT include any text outside the JSON block.
```json
{{
    "selected_choices": {{
        "Q1 or Q2": "Q1", // or "Q2" based on student's answers
        "Q3 or Q4": "Q4"  // or "Q3" based on student's answers
        // ... add other choices as needed based on your paper structure
    }},
    "mapped_answers": [
        {{
            "question_id": "Q1a", // e.g., "Q1a", "Q1b", "Q4b"
            "question_text": "Define \\"Quality\\" as viewed by different stakeholders of software development and usage.",
            "student_answer_extracted": "Q1 a) Quality: The definition of quality differs from person to person. but its major criteria of determining is for based upon the factor like..." // The actual extracted text
        }},
        // ... more mapped answers for other attempted question parts
    ]
}}
```
Ensure the extracted "student_answer_extracted" is comprehensive for the identified part.
"""

EVALUATION_PROMPT = """
You are an experienced academic evaluator for an engineering course, specializing in software quality and data mining.
Your task is to thoroughly evaluate a student's answer to a specific question based on academic standards, similar to AICTE guidelines.

**Important Scoring Instruction:** Be slightly lenient in your scoring. Prioritize awarding marks for correct concepts even if phrasing is imperfect or minor details are missing. Focus on what the student *did* include correctly and demonstrate understanding. Give the benefit of the doubt on minor omissions if the core understanding is demonstrated.

Follow this Chain of Thought (COT) process to produce your evaluation:

**Chain of Thought (COT) Steps:**
1.  **Analyze the Question:** Break down the question to understand its core requirements, keywords, and what exactly is being asked (e.g., define, explain, compare, discuss, justify).
2.  **Analyze the Student's Answer:** Read the student's response carefully. Identify the main points, arguments, and examples provided. Note any initial strengths or weaknesses.
3.  **Evaluate against Rubric with Justification:** Apply the following criteria to assess the student's answer and assign a score for each. Provide a concise, specific justification explaining *why* that score was given, referencing parts of the student's answer or lack thereof.
    *   **Content Accuracy (correctness of facts, definitions, concepts):** (Max: {content_accuracy} marks)
    *   **Completeness & Depth (coverage of all parts of the question, sufficient detail):** (Max: {completeness_depth} marks)
    *   **Relevance & Focus (directness to question, no irrelevant information):** (Max: {relevance_focus} marks)
    *   **Clarity & Coherence (logical flow, readability, proper use of terminology):** (Max: {clarity_coherence} marks)
4.  **Calculate Overall Score:** Sum up the scores from each rubric criterion.
5.  **Provide Summary Feedback:** Offer constructive feedback summarizing the strengths and areas for improvement.

**Question to Evaluate:**
```
{question_text}
```
(This question is worth {max_marks} marks.)

**Student's Answer:**
```
{student_answer_text}
```

**Your Output Format (Strictly adhere to this JSON structure):**
```json
{{
    "evaluation": {{
        "question_text": "{question_text}",
        "student_answer_extracted": "{student_answer_text}",
        "max_marks": {max_marks},
        "rubric_scores": {{
            "content_accuracy": {{"score": [Score_Accuracy], "justification": "[Justification_Accuracy]"}},
            "completeness_depth": {{"score": [Score_Completeness], "justification": "[Justification_Completeness]"}},
            "relevance_focus": {{"score": [Score_Relevance], "justification": "[Justification_Relevance]"}},
            "clarity_coherence": {{"score": [Score_Clarity], "justification": "[Justification_Clarity]"}}
        }},
        "overall_score": [Total_Score],
        "percentage": [Percentage],
        "feedback": {{
            "strengths": "[List key strengths of the answer]",
            "improvements": "[Suggest specific areas for improvement]",
            "summary": "[Overall assessment and grade justification]"
        }}
    }}
}}
```
"""

# Simple rubric-based evaluation (multi-agent professor prompts removed as per requirements)

# Question paper parsing prompt
QUESTION_PARSING_PROMPT = """You are a meticulous exam-paper parser.
From the supplied image you will extract ONLY the exam questions, choices,
sub-parts and marks, returning **valid JSON** that conforms exactly to the
schema below.

### Schema ###
[
  {
    "choice": "Q1 or Q2",        # The heading when questions are offered as options
    "options": [
      {
        "id": "Q1",
        "parts": [
          { "part_id": "a", "question_text": "...", "marks": 5 },
          { "part_id": "b", "question_text": "...", "marks": 5 }
        ]
      },
      { "id": "Q2", "parts": [ ... ] }
    ]
  },
  ...
]

*If a question is NOT an either/or option, set `choice` to the question id
  (e.g. "Q5") and put that single question inside `options`.
*Strip any bracket symbols around part ids.
*Do not include any text or markdown fences (like ```json) outside the JSON array.
*Extract question text verbatim as it appears in the image.
*Identify marks/points for each question or sub-part from brackets or explicit mentions.
*Preserve the original question structure and numbering.
Return nothing else but valid JSON.""" 